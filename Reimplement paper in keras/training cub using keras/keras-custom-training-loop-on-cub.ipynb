{"metadata":{"colab":{"collapsed_sections":["EK5ZjqZEAI5F","ThFX0m_Llmp7","B3w5RE4CwYuv","ffgNx7h3wd1U","_CpCil-rDqHG","gT5gPrISz6Rs","6wvLU_CrEJYt","5UAZ0fmEQAF1","WEiicGpZKA1F","5eeefVlNx1Ir"],"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"01d421cb601b4e0894739ebcb30388ef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e072bdb7858041aaaf9ca627f618fc6f","IPY_MODEL_b6ae5e4da228424d802c7829c6ebe63a","IPY_MODEL_abee0018643949f7bef9be2ca82d5f5a"],"layout":"IPY_MODEL_794978d95bca43f0b3121351a81b11bd"}},"e072bdb7858041aaaf9ca627f618fc6f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1e1671dda044543a15e44525e86d218","placeholder":"​","style":"IPY_MODEL_4274be089d7c41e9b970cc608324f293","value":"Dl Completed...: 100%"}},"b6ae5e4da228424d802c7829c6ebe63a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_43efe2abbc23484587f714b3f2d4cec0","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7291e2efd8c84fb682501d60d41383ed","value":1}},"abee0018643949f7bef9be2ca82d5f5a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b4751c780af41a799ce2b9b466e0efc","placeholder":"​","style":"IPY_MODEL_39e1d4fdcdf94f2a8ae3a155dd6c2919","value":" 2/2 [02:11&lt;00:00, 47.29s/ url]"}},"794978d95bca43f0b3121351a81b11bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1e1671dda044543a15e44525e86d218":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4274be089d7c41e9b970cc608324f293":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"43efe2abbc23484587f714b3f2d4cec0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"7291e2efd8c84fb682501d60d41383ed":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6b4751c780af41a799ce2b9b466e0efc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39e1d4fdcdf94f2a8ae3a155dd6c2919":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e32a4069a534759961a0ba7db416803":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6bf5be7bdf854c78baff22bfd2288114","IPY_MODEL_696401c8aaa14a928a2760d2a5770fea","IPY_MODEL_751d1771c4ee42f9b45c044212b52623"],"layout":"IPY_MODEL_d736d9d634a24b9f8c9b086fda130eb0"}},"6bf5be7bdf854c78baff22bfd2288114":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52bfb1e3ea114836b4cc08d0b49896a7","placeholder":"​","style":"IPY_MODEL_80a1667210b34c38a0ce0339da344028","value":"Dl Size...: 100%"}},"696401c8aaa14a928a2760d2a5770fea":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b0b63c74f88425a8a179fd088692516","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c452de1303404d1c91d031eae7a5d527","value":1}},"751d1771c4ee42f9b45c044212b52623":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57707778c8c946b4b352d692737a15b8","placeholder":"​","style":"IPY_MODEL_e53e5b76d6534326bc2e858cca4edc9f","value":" 1218/1218 [02:11&lt;00:00, 16.60 MiB/s]"}},"d736d9d634a24b9f8c9b086fda130eb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52bfb1e3ea114836b4cc08d0b49896a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80a1667210b34c38a0ce0339da344028":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8b0b63c74f88425a8a179fd088692516":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"c452de1303404d1c91d031eae7a5d527":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"57707778c8c946b4b352d692737a15b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e53e5b76d6534326bc2e858cca4edc9f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"03147b06c4024fdd96fe24039b6dbf68":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_46ce8a670f1b47f78b6d4f46f1b34376","IPY_MODEL_e7310fd52fe94c4ab1f5abea656d0c67","IPY_MODEL_f650b09e2cc649fd8ba08784aa47b525"],"layout":"IPY_MODEL_1523e9c5b9564cbca9a35a36e89a3441"}},"46ce8a670f1b47f78b6d4f46f1b34376":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d04da922b3734a139806846ab4b74b6f","placeholder":"​","style":"IPY_MODEL_8e3967ade9e34a01af5c031a909c08c6","value":"Extraction completed...: 100%"}},"e7310fd52fe94c4ab1f5abea656d0c67":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc8e3eec38294084a0f4fd8187fef966","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e2b0bff910da4d1db102e53dcb8aad3b","value":1}},"f650b09e2cc649fd8ba08784aa47b525":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_910114b538304664a34e84f65dfb5eb8","placeholder":"​","style":"IPY_MODEL_118b68f14f53453d8c1eaa2a2270552b","value":" 12801/12801 [02:11&lt;00:00, 1009.73 file/s]"}},"1523e9c5b9564cbca9a35a36e89a3441":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d04da922b3734a139806846ab4b74b6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e3967ade9e34a01af5c031a909c08c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc8e3eec38294084a0f4fd8187fef966":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"e2b0bff910da4d1db102e53dcb8aad3b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"910114b538304664a34e84f65dfb5eb8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"118b68f14f53453d8c1eaa2a2270552b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install keras_core keras_cv munch tensorflow -U","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nnQ5pPuV8GM1","outputId":"fd765cf0-750e-4f56-8e23-1910c7ba59a0","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ps -ef","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\nimport cv2\nimport keras.backend as K\nimport keras_cv\nimport re\nimport tqdm\nimport os\nimport copy\n\nfrom tensorflow import keras\nfrom keras_cv import visualization","metadata":{"id":"q8S_DYhn5Vdi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9d45e387-49e6-4cf1-ab2e-8d6d75529566","execution":{"iopub.status.busy":"2023-10-14T00:47:52.787288Z","iopub.execute_input":"2023-10-14T00:47:52.788024Z","iopub.status.idle":"2023-10-14T00:47:58.549604Z","shell.execute_reply.started":"2023-10-14T00:47:52.787988Z","shell.execute_reply":"2023-10-14T00:47:58.548493Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2023-10-14 00:47:53.440279: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2023-10-14 00:47:53.440343: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2023-10-14 00:47:53.440396: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"Using TensorFlow backend\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Main","metadata":{"id":"QvC4TqeH-2dD"}},{"cell_type":"markdown","source":"## Download and extract data","metadata":{"id":"EK5ZjqZEAI5F"}},{"cell_type":"code","source":"# cub_200_2011_url = 'https://drive.google.com/uc?export=download&id=1hbzc_P1FuxMkcabkgn9ZKinBwW683j45'\n\ncub_200_2011_url = 'https://data.caltech.edu/records/65de6-vp158/files/CUB_200_2011.tgz'\ncub_v2_url = 'https://drive.google.com/u/0/uc?id=1U6cwKHS65wayT9FFvoLIA8cn1k0Ot2M1&export=download'\n\ndl_manager = tfds.download.DownloadManager(\n    download_dir='/kaggle/working/dataset',\n    extract_dir='/kaggle/working/dataset',\n    dataset_name='CUB'\n)\n\ndata_dirs = dl_manager.download_and_extract([\n    cub_200_2011_url,\n    cub_v2_url\n])","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0,"referenced_widgets":["01d421cb601b4e0894739ebcb30388ef","e072bdb7858041aaaf9ca627f618fc6f","b6ae5e4da228424d802c7829c6ebe63a","abee0018643949f7bef9be2ca82d5f5a","794978d95bca43f0b3121351a81b11bd","a1e1671dda044543a15e44525e86d218","4274be089d7c41e9b970cc608324f293","43efe2abbc23484587f714b3f2d4cec0","7291e2efd8c84fb682501d60d41383ed","6b4751c780af41a799ce2b9b466e0efc","39e1d4fdcdf94f2a8ae3a155dd6c2919","2e32a4069a534759961a0ba7db416803","6bf5be7bdf854c78baff22bfd2288114","696401c8aaa14a928a2760d2a5770fea","751d1771c4ee42f9b45c044212b52623","d736d9d634a24b9f8c9b086fda130eb0","52bfb1e3ea114836b4cc08d0b49896a7","80a1667210b34c38a0ce0339da344028","8b0b63c74f88425a8a179fd088692516","c452de1303404d1c91d031eae7a5d527","57707778c8c946b4b352d692737a15b8","e53e5b76d6534326bc2e858cca4edc9f","03147b06c4024fdd96fe24039b6dbf68","46ce8a670f1b47f78b6d4f46f1b34376","e7310fd52fe94c4ab1f5abea656d0c67","f650b09e2cc649fd8ba08784aa47b525","1523e9c5b9564cbca9a35a36e89a3441","d04da922b3734a139806846ab4b74b6f","8e3967ade9e34a01af5c031a909c08c6","dc8e3eec38294084a0f4fd8187fef966","e2b0bff910da4d1db102e53dcb8aad3b","910114b538304664a34e84f65dfb5eb8","118b68f14f53453d8c1eaa2a2270552b"]},"id":"XUvISd5iYhpw","outputId":"f451d411-80b6-4878-decb-7210b5381848","execution":{"iopub.status.busy":"2023-10-14T00:47:58.551761Z","iopub.execute_input":"2023-10-14T00:47:58.553141Z","iopub.status.idle":"2023-10-14T00:47:58.937940Z","shell.execute_reply.started":"2023-10-14T00:47:58.553100Z","shell.execute_reply":"2023-10-14T00:47:58.936926Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Dl Completed...: 0 url [00:00, ? url/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"860e7218a98441e78c165a34d2222c4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Size...: 0 MiB [00:00, ? MiB/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4cbda7af47040f4bcfbd4b41a04d3ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extraction completed...: 0 file [00:00, ? file/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9eb07f4f7e524bdab0d7fe846ace94ae"}},"metadata":{}}]},{"cell_type":"code","source":"!rsync -av /kaggle/working/dataset/TAR.u_0_ucid_1U6cwKHS65wayT9FFvoLI_export_downloadtucVXoFCaF1zLeWZ2WFyzjnY-FlfYz1pOwkzbO6x5Ds/ /kaggle/working/CUB\n!rsync -av /kaggle/working/dataset/TAR_GZ.data.calt.edu_reco_65de-vp15_file_CUB_200_ahP8ECcIFAKOrAGhfweAoyw8ALO8u5-8goN4zja5p1g.tgz/CUB_200_2011/images/ /kaggle/working/CUB","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7veHlC8dxqUj","outputId":"7fc80415-d4ba-4475-a157-183086442bb0","scrolled":true,"execution":{"iopub.status.busy":"2023-10-14T00:47:58.939444Z","iopub.execute_input":"2023-10-14T00:47:58.940425Z","iopub.status.idle":"2023-10-14T00:48:01.455662Z","shell.execute_reply.started":"2023-10-14T00:47:58.940390Z","shell.execute_reply":"2023-10-14T00:48:01.454297Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"sending incremental file list\n./\n001.Black_footed_Albatross/\n002.Laysan_Albatross/\n003.Sooty_Albatross/\n004.Groove_billed_Ani/\n005.Crested_Auklet/\n006.Least_Auklet/\n007.Parakeet_Auklet/\n008.Rhinoceros_Auklet/\n009.Brewer_Blackbird/\n010.Red_winged_Blackbird/\n011.Rusty_Blackbird/\n012.Yellow_headed_Blackbird/\n013.Bobolink/\n014.Indigo_Bunting/\n015.Lazuli_Bunting/\n016.Painted_Bunting/\n017.Cardinal/\n018.Spotted_Catbird/\n019.Gray_Catbird/\n020.Yellow_breasted_Chat/\n021.Eastern_Towhee/\n022.Chuck_will_Widow/\n023.Brandt_Cormorant/\n024.Red_faced_Cormorant/\n025.Pelagic_Cormorant/\n026.Bronzed_Cowbird/\n027.Shiny_Cowbird/\n028.Brown_Creeper/\n029.American_Crow/\n030.Fish_Crow/\n031.Black_billed_Cuckoo/\n032.Mangrove_Cuckoo/\n033.Yellow_billed_Cuckoo/\n034.Gray_crowned_Rosy_Finch/\n035.Purple_Finch/\n036.Northern_Flicker/\n037.Acadian_Flycatcher/\n038.Great_Crested_Flycatcher/\n039.Least_Flycatcher/\n040.Olive_sided_Flycatcher/\n041.Scissor_tailed_Flycatcher/\n042.Vermilion_Flycatcher/\n043.Yellow_bellied_Flycatcher/\n044.Frigatebird/\n045.Northern_Fulmar/\n046.Gadwall/\n047.American_Goldfinch/\n048.European_Goldfinch/\n049.Boat_tailed_Grackle/\n050.Eared_Grebe/\n051.Horned_Grebe/\n052.Pied_billed_Grebe/\n053.Western_Grebe/\n054.Blue_Grosbeak/\n055.Evening_Grosbeak/\n056.Pine_Grosbeak/\n057.Rose_breasted_Grosbeak/\n058.Pigeon_Guillemot/\n059.California_Gull/\n060.Glaucous_winged_Gull/\n061.Heermann_Gull/\n062.Herring_Gull/\n063.Ivory_Gull/\n064.Ring_billed_Gull/\n065.Slaty_backed_Gull/\n066.Western_Gull/\n067.Anna_Hummingbird/\n068.Ruby_throated_Hummingbird/\n069.Rufous_Hummingbird/\n070.Green_Violetear/\n071.Long_tailed_Jaeger/\n072.Pomarine_Jaeger/\n073.Blue_Jay/\n074.Florida_Jay/\n075.Green_Jay/\n076.Dark_eyed_Junco/\n077.Tropical_Kingbird/\n078.Gray_Kingbird/\n079.Belted_Kingfisher/\n080.Green_Kingfisher/\n081.Pied_Kingfisher/\n082.Ringed_Kingfisher/\n083.White_breasted_Kingfisher/\n084.Red_legged_Kittiwake/\n085.Horned_Lark/\n086.Pacific_Loon/\n087.Mallard/\n088.Western_Meadowlark/\n089.Hooded_Merganser/\n090.Red_breasted_Merganser/\n091.Mockingbird/\n092.Nighthawk/\n093.Clark_Nutcracker/\n094.White_breasted_Nuthatch/\n095.Baltimore_Oriole/\n096.Hooded_Oriole/\n097.Orchard_Oriole/\n098.Scott_Oriole/\n099.Ovenbird/\n100.Brown_Pelican/\n101.White_Pelican/\n102.Western_Wood_Pewee/\n103.Sayornis/\n104.American_Pipit/\n105.Whip_poor_Will/\n106.Horned_Puffin/\n107.Common_Raven/\n108.White_necked_Raven/\n109.American_Redstart/\n110.Geococcyx/\n111.Loggerhead_Shrike/\n112.Great_Grey_Shrike/\n113.Baird_Sparrow/\n114.Black_throated_Sparrow/\n115.Brewer_Sparrow/\n116.Chipping_Sparrow/\n117.Clay_colored_Sparrow/\n118.House_Sparrow/\n119.Field_Sparrow/\n120.Fox_Sparrow/\n121.Grasshopper_Sparrow/\n122.Harris_Sparrow/\n123.Henslow_Sparrow/\n124.Le_Conte_Sparrow/\n125.Lincoln_Sparrow/\n126.Nelson_Sharp_tailed_Sparrow/\n127.Savannah_Sparrow/\n128.Seaside_Sparrow/\n129.Song_Sparrow/\n130.Tree_Sparrow/\n131.Vesper_Sparrow/\n132.White_crowned_Sparrow/\n133.White_throated_Sparrow/\n134.Cape_Glossy_Starling/\n135.Bank_Swallow/\n136.Barn_Swallow/\n137.Cliff_Swallow/\n138.Tree_Swallow/\n139.Scarlet_Tanager/\n140.Summer_Tanager/\n141.Artic_Tern/\n142.Black_Tern/\n143.Caspian_Tern/\n144.Common_Tern/\n145.Elegant_Tern/\n146.Forsters_Tern/\n147.Least_Tern/\n148.Green_tailed_Towhee/\n149.Brown_Thrasher/\n150.Sage_Thrasher/\n151.Black_capped_Vireo/\n152.Blue_headed_Vireo/\n153.Philadelphia_Vireo/\n154.Red_eyed_Vireo/\n155.Warbling_Vireo/\n156.White_eyed_Vireo/\n157.Yellow_throated_Vireo/\n158.Bay_breasted_Warbler/\n159.Black_and_white_Warbler/\n160.Black_throated_Blue_Warbler/\n161.Blue_winged_Warbler/\n162.Canada_Warbler/\n163.Cape_May_Warbler/\n164.Cerulean_Warbler/\n165.Chestnut_sided_Warbler/\n166.Golden_winged_Warbler/\n167.Hooded_Warbler/\n168.Kentucky_Warbler/\n169.Magnolia_Warbler/\n170.Mourning_Warbler/\n171.Myrtle_Warbler/\n172.Nashville_Warbler/\n173.Orange_crowned_Warbler/\n174.Palm_Warbler/\n175.Pine_Warbler/\n176.Prairie_Warbler/\n177.Prothonotary_Warbler/\n178.Swainson_Warbler/\n179.Tennessee_Warbler/\n180.Wilson_Warbler/\n181.Worm_eating_Warbler/\n182.Yellow_Warbler/\n183.Northern_Waterthrush/\n184.Louisiana_Waterthrush/\n185.Bohemian_Waxwing/\n186.Cedar_Waxwing/\n187.American_Three_toed_Woodpecker/\n188.Pileated_Woodpecker/\n189.Red_bellied_Woodpecker/\n190.Red_cockaded_Woodpecker/\n191.Red_headed_Woodpecker/\n192.Downy_Woodpecker/\n193.Bewick_Wren/\n194.Cactus_Wren/\n195.Carolina_Wren/\n196.House_Wren/\n197.Marsh_Wren/\n198.Rock_Wren/\n199.Winter_Wren/\n200.Common_Yellowthroat/\n\nsent 38,760 bytes  received 819 bytes  79,158.00 bytes/sec\ntotal size is 126,922,574  speedup is 3,206.82\nsending incremental file list\n./\n001.Black_footed_Albatross/\n002.Laysan_Albatross/\n003.Sooty_Albatross/\n004.Groove_billed_Ani/\n005.Crested_Auklet/\n006.Least_Auklet/\n007.Parakeet_Auklet/\n008.Rhinoceros_Auklet/\n009.Brewer_Blackbird/\n010.Red_winged_Blackbird/\n011.Rusty_Blackbird/\n012.Yellow_headed_Blackbird/\n013.Bobolink/\n014.Indigo_Bunting/\n015.Lazuli_Bunting/\n016.Painted_Bunting/\n017.Cardinal/\n018.Spotted_Catbird/\n019.Gray_Catbird/\n020.Yellow_breasted_Chat/\n021.Eastern_Towhee/\n022.Chuck_will_Widow/\n023.Brandt_Cormorant/\n024.Red_faced_Cormorant/\n025.Pelagic_Cormorant/\n026.Bronzed_Cowbird/\n027.Shiny_Cowbird/\n028.Brown_Creeper/\n029.American_Crow/\n030.Fish_Crow/\n031.Black_billed_Cuckoo/\n032.Mangrove_Cuckoo/\n033.Yellow_billed_Cuckoo/\n034.Gray_crowned_Rosy_Finch/\n035.Purple_Finch/\n036.Northern_Flicker/\n037.Acadian_Flycatcher/\n038.Great_Crested_Flycatcher/\n039.Least_Flycatcher/\n040.Olive_sided_Flycatcher/\n041.Scissor_tailed_Flycatcher/\n042.Vermilion_Flycatcher/\n043.Yellow_bellied_Flycatcher/\n044.Frigatebird/\n045.Northern_Fulmar/\n046.Gadwall/\n047.American_Goldfinch/\n048.European_Goldfinch/\n049.Boat_tailed_Grackle/\n050.Eared_Grebe/\n051.Horned_Grebe/\n052.Pied_billed_Grebe/\n053.Western_Grebe/\n054.Blue_Grosbeak/\n055.Evening_Grosbeak/\n056.Pine_Grosbeak/\n057.Rose_breasted_Grosbeak/\n058.Pigeon_Guillemot/\n059.California_Gull/\n060.Glaucous_winged_Gull/\n061.Heermann_Gull/\n062.Herring_Gull/\n063.Ivory_Gull/\n064.Ring_billed_Gull/\n065.Slaty_backed_Gull/\n066.Western_Gull/\n067.Anna_Hummingbird/\n068.Ruby_throated_Hummingbird/\n069.Rufous_Hummingbird/\n070.Green_Violetear/\n071.Long_tailed_Jaeger/\n072.Pomarine_Jaeger/\n073.Blue_Jay/\n074.Florida_Jay/\n075.Green_Jay/\n076.Dark_eyed_Junco/\n077.Tropical_Kingbird/\n078.Gray_Kingbird/\n079.Belted_Kingfisher/\n080.Green_Kingfisher/\n081.Pied_Kingfisher/\n082.Ringed_Kingfisher/\n083.White_breasted_Kingfisher/\n084.Red_legged_Kittiwake/\n085.Horned_Lark/\n086.Pacific_Loon/\n087.Mallard/\n088.Western_Meadowlark/\n089.Hooded_Merganser/\n090.Red_breasted_Merganser/\n091.Mockingbird/\n092.Nighthawk/\n093.Clark_Nutcracker/\n094.White_breasted_Nuthatch/\n095.Baltimore_Oriole/\n096.Hooded_Oriole/\n097.Orchard_Oriole/\n098.Scott_Oriole/\n099.Ovenbird/\n100.Brown_Pelican/\n101.White_Pelican/\n102.Western_Wood_Pewee/\n103.Sayornis/\n104.American_Pipit/\n105.Whip_poor_Will/\n106.Horned_Puffin/\n107.Common_Raven/\n108.White_necked_Raven/\n109.American_Redstart/\n110.Geococcyx/\n111.Loggerhead_Shrike/\n112.Great_Grey_Shrike/\n113.Baird_Sparrow/\n114.Black_throated_Sparrow/\n115.Brewer_Sparrow/\n116.Chipping_Sparrow/\n117.Clay_colored_Sparrow/\n118.House_Sparrow/\n119.Field_Sparrow/\n120.Fox_Sparrow/\n121.Grasshopper_Sparrow/\n122.Harris_Sparrow/\n123.Henslow_Sparrow/\n124.Le_Conte_Sparrow/\n125.Lincoln_Sparrow/\n126.Nelson_Sharp_tailed_Sparrow/\n127.Savannah_Sparrow/\n128.Seaside_Sparrow/\n129.Song_Sparrow/\n130.Tree_Sparrow/\n131.Vesper_Sparrow/\n132.White_crowned_Sparrow/\n133.White_throated_Sparrow/\n134.Cape_Glossy_Starling/\n135.Bank_Swallow/\n136.Barn_Swallow/\n137.Cliff_Swallow/\n138.Tree_Swallow/\n139.Scarlet_Tanager/\n140.Summer_Tanager/\n141.Artic_Tern/\n142.Black_Tern/\n143.Caspian_Tern/\n144.Common_Tern/\n145.Elegant_Tern/\n146.Forsters_Tern/\n147.Least_Tern/\n148.Green_tailed_Towhee/\n149.Brown_Thrasher/\n150.Sage_Thrasher/\n151.Black_capped_Vireo/\n152.Blue_headed_Vireo/\n153.Philadelphia_Vireo/\n154.Red_eyed_Vireo/\n155.Warbling_Vireo/\n156.White_eyed_Vireo/\n157.Yellow_throated_Vireo/\n158.Bay_breasted_Warbler/\n159.Black_and_white_Warbler/\n160.Black_throated_Blue_Warbler/\n161.Blue_winged_Warbler/\n162.Canada_Warbler/\n163.Cape_May_Warbler/\n164.Cerulean_Warbler/\n165.Chestnut_sided_Warbler/\n166.Golden_winged_Warbler/\n167.Hooded_Warbler/\n168.Kentucky_Warbler/\n169.Magnolia_Warbler/\n170.Mourning_Warbler/\n171.Myrtle_Warbler/\n172.Nashville_Warbler/\n173.Orange_crowned_Warbler/\n174.Palm_Warbler/\n175.Pine_Warbler/\n176.Prairie_Warbler/\n177.Prothonotary_Warbler/\n178.Swainson_Warbler/\n179.Tennessee_Warbler/\n180.Wilson_Warbler/\n181.Worm_eating_Warbler/\n182.Yellow_Warbler/\n183.Northern_Waterthrush/\n184.Louisiana_Waterthrush/\n185.Bohemian_Waxwing/\n186.Cedar_Waxwing/\n187.American_Three_toed_Woodpecker/\n188.Pileated_Woodpecker/\n189.Red_bellied_Woodpecker/\n190.Red_cockaded_Woodpecker/\n191.Red_headed_Woodpecker/\n192.Downy_Woodpecker/\n193.Bewick_Wren/\n194.Cactus_Wren/\n195.Carolina_Wren/\n196.House_Wren/\n197.Marsh_Wren/\n198.Rock_Wren/\n199.Winter_Wren/\n200.Common_Yellowthroat/\n\nsent 305,471 bytes  received 863 bytes  612,668.00 bytes/sec\ntotal size is 1,150,650,529  speedup is 3,756.20\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working\n!git clone https://github.com/clovaai/wsolevaluation.git","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h7ZB9adt7yCC","outputId":"b761280e-cb44-4222-f447-0e22a8f378a9","execution":{"iopub.status.busy":"2023-10-14T00:48:01.459247Z","iopub.execute_input":"2023-10-14T00:48:01.460374Z","iopub.status.idle":"2023-10-14T00:48:02.535419Z","shell.execute_reply.started":"2023-10-14T00:48:01.460331Z","shell.execute_reply":"2023-10-14T00:48:02.534086Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/working\nfatal: destination path 'wsolevaluation' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load data","metadata":{"id":"8NommkOo_q58"}},{"cell_type":"code","source":"input_shape = (224, 224, 3)\nnum_classes = 200\nbatch_size = 32\ndrop_threshold = 0.8\ndrop_prob = 0.25\nsim_fg_thres = 0.4\nsim_bg_thres = 0.2\nloss_ratio_sim = 0.5\nloss_ratio_norm = 0.05\nloss_ratio_drop = 2.0\n\nimg_dir='/kaggle/working/CUB'","metadata":{"id":"KCVXqGL58dQE","execution":{"iopub.status.busy":"2023-10-14T00:48:02.537328Z","iopub.execute_input":"2023-10-14T00:48:02.537800Z","iopub.status.idle":"2023-10-14T00:48:02.545084Z","shell.execute_reply.started":"2023-10-14T00:48:02.537761Z","shell.execute_reply":"2023-10-14T00:48:02.543950Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_labels = tf.data.TextLineDataset('/kaggle/working/wsolevaluation/metadata/CUB/train/class_labels.txt')\ntrain_ids = tf.data.TextLineDataset('/kaggle/working/wsolevaluation/metadata/CUB/train/image_ids.txt')\ntrain_ds = tf.data.Dataset.zip(train_ids, train_labels)\n\nval_labels = tf.data.TextLineDataset('/kaggle/working/wsolevaluation/metadata/CUB/val/class_labels.txt')\nval_ids = tf.data.TextLineDataset('/kaggle/working/wsolevaluation/metadata/CUB/val/image_ids.txt')\nval_ds = tf.data.Dataset.zip(val_ids, val_labels)","metadata":{"id":"kyqRPI9I5EDp","execution":{"iopub.status.busy":"2023-10-14T00:48:02.546870Z","iopub.execute_input":"2023-10-14T00:48:02.547612Z","iopub.status.idle":"2023-10-14T00:48:05.874464Z","shell.execute_reply.started":"2023-10-14T00:48:02.547576Z","shell.execute_reply":"2023-10-14T00:48:05.873412Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def preprocess_data(id, label):\n    # Load and resize image\n    raw = tf.io.read_file(tf.strings.join([img_dir, id], separator='/'))\n    image = tf.image.decode_jpeg(raw, channels=3)\n    image = tf.image.resize(image, input_shape[:2])\n\n    # Convert index label to binary array [0, 0, 1, ..., 0, 0]\n    label =  tf.strings.to_number(tf.strings.split(label, ',')[-1], out_type=tf.int32)\n    label = tf.one_hot(label, num_classes)\n\n    return image, {\n        'id': id,\n        'classes': label\n    }","metadata":{"id":"aYliydL67EZN","execution":{"iopub.status.busy":"2023-10-14T00:48:05.875959Z","iopub.execute_input":"2023-10-14T00:48:05.876325Z","iopub.status.idle":"2023-10-14T00:48:05.882845Z","shell.execute_reply.started":"2023-10-14T00:48:05.876294Z","shell.execute_reply":"2023-10-14T00:48:05.881772Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/wsolevaluation/metadata/CUB/train/image_ids.txt', 'r') as f:\n    train_num_examples = len(f.readlines())\n    print(train_num_examples)\n\nwith open('/kaggle/working/wsolevaluation/metadata/CUB/val/image_ids.txt', 'r') as f:\n    val_num_examples = len(f.readlines())\n    print(val_num_examples)\n\nwith open('/kaggle/working/wsolevaluation/metadata/CUB/test/image_ids.txt', 'r') as f:\n    test_num_examples = len(f.readlines())\n    print(test_num_examples)\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x3qYVdHDRlfr","outputId":"286752aa-e973-4c9d-bfba-4b093bde1871","execution":{"iopub.status.busy":"2023-10-14T00:48:05.884496Z","iopub.execute_input":"2023-10-14T00:48:05.885590Z","iopub.status.idle":"2023-10-14T00:48:05.901415Z","shell.execute_reply.started":"2023-10-14T00:48:05.885558Z","shell.execute_reply":"2023-10-14T00:48:05.900109Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"5994\n1000\n5794\n","output_type":"stream"}]},{"cell_type":"code","source":"train_ds = train_ds.map(\n    preprocess_data,\n    num_parallel_calls=tf.data.AUTOTUNE\n)\ntrain_ds = train_ds.cache()\ntrain_ds = train_ds.shuffle(train_num_examples)\ntrain_ds = train_ds.batch(batch_size=batch_size)\ntrain_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n\nval_ds = val_ds.map(\n    preprocess_data,\n    num_parallel_calls=tf.data.AUTOTUNE\n)\nval_ds = val_ds.cache()\nval_ds = val_ds.batch(batch_size=batch_size)\nval_ds = val_ds.prefetch(tf.data.AUTOTUNE)","metadata":{"id":"XH9wkZ-NCikm","execution":{"iopub.status.busy":"2023-10-14T00:48:05.903495Z","iopub.execute_input":"2023-10-14T00:48:05.904237Z","iopub.status.idle":"2023-10-14T00:48:06.127236Z","shell.execute_reply.started":"2023-10-14T00:48:05.904131Z","shell.execute_reply":"2023-10-14T00:48:06.126216Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Define layer and loss function","metadata":{"id":"ThFX0m_Llmp7"}},{"cell_type":"markdown","source":"### Attentive dropout layer","metadata":{"id":"B3w5RE4CwYuv"}},{"cell_type":"code","source":"class AttentiveDropout(keras.layers.Layer):\n    def __init__(self, drop_threshold=0.7, drop_prob=1.):\n        super(AttentiveDropout, self).__init__()\n\n        if not (0 <= drop_threshold <= 1):\n            raise ValueError(\"Drop threshold must be in range [0, 1].\")\n\n        if not (0 <= drop_prob <= 1):\n            raise ValueError(\"Drop probability must be in range [0, 1].\")\n\n        self.drop_threshold = drop_threshold\n        self.drop_prob = drop_prob\n        self.globalmaxpool = keras.layers.GlobalMaxPooling2D(keepdims=True)\n\n    def call(self, input):\n        m_avg = K.mean(input, axis=3, keepdims=True)\n        thres = tf.broadcast_to(self.globalmaxpool(m_avg) * self.drop_threshold, K.shape(m_avg))\n\n        dropped_mask = tf.where(m_avg > thres, tf.zeros_like(m_avg), tf.ones_like(m_avg))\n        random_tensor = tf.random.uniform(shape=tf.shape(m_avg), dtype=tf.float32)\n        dropped_mask = tf.where(random_tensor <= self.drop_prob, dropped_mask, tf.ones_like(dropped_mask))\n\n        erased_input = input * dropped_mask\n\n        return erased_input","metadata":{"id":"dCf_-7yjhG4R","execution":{"iopub.status.busy":"2023-10-14T00:48:06.131165Z","iopub.execute_input":"2023-10-14T00:48:06.131498Z","iopub.status.idle":"2023-10-14T00:48:06.139506Z","shell.execute_reply.started":"2023-10-14T00:48:06.131473Z","shell.execute_reply":"2023-10-14T00:48:06.138512Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Define loss","metadata":{"id":"ffgNx7h3wd1U"}},{"cell_type":"code","source":"@tf.function\ndef get_sim(feature, weight):\n    # Calculate cosine similarity between weight and feature\n    x_normalized = K.l2_normalize(feature, axis=3)\n    weight_normalized = K.l2_normalize(weight, axis=0)\n\n    # Reshape the normalized weights to have shape (1, 1, num_input_channels, num_output_channels)\n    weight_normalized_reshaped = K.reshape(weight_normalized, (1, 1, weight_normalized.shape[0], -1))\n\n    # weight.shape[1] = num_classes in tf\n    return keras.layers.Conv2D(filters=weight.shape[1], kernel_size=(1, 1), use_bias=False).convolution_op(x_normalized, weight_normalized_reshaped)\n\n@tf.function\ndef get_loss_alignment(\n        feature,\n        weight,\n        target,\n        sim_bg_thres,\n        sim_fg_thres,\n        eps=1e-15\n    ):\n    sim = get_sim(feature, weight)\n    # Flatten all class similarity map per sample\n    sim_flat = tf.keras.layers.Reshape((-1, num_classes))(sim) # result shape B 784 num_classes\n\n    feature_norm = tf.norm(feature, axis=3)\n\n    # Minmax normalize\n    feature_norm_flatten = K.batch_flatten(feature_norm)\n    min = K.min(feature_norm_flatten, axis=1)\n    max = K.max(feature_norm_flatten, axis=1)\n    feature_norm_minmax = K.reshape(\n        (feature_norm_flatten - min[:, tf.newaxis]) / (max[:, tf.newaxis] - min[:, tf.newaxis]),\n        K.shape(feature_norm)\n    )\n\n    feature_norm_minmax_flat = K.batch_flatten(feature_norm_minmax)\n\n    # Sim loss\n    sim_fg = K.cast(feature_norm_minmax_flat > 0.4, dtype='float32')\n    sim_bg = K.cast(feature_norm_minmax_flat < 0.2, dtype='float32')\n\n    sim_fg_mean = K.sum(sim_fg[..., tf.newaxis] * sim_flat, axis=1) / (K.sum(sim_fg, axis=1) + eps)[:, tf.newaxis]\n    sim_bg_mean = K.sum(sim_bg[..., tf.newaxis] * sim_flat, axis=1) / (K.sum(sim_bg, axis=1) + eps)[:, tf.newaxis]\n\n    loss_sim = K.cast(target, dtype='float32') * (sim_bg_mean - sim_fg_mean)\n\n    # per sample loss sim\n    loss_sim = K.sum(loss_sim, axis=1) / K.cast(tf.math.count_nonzero(loss_sim, axis=1), dtype='float32')\n    # scalar loss sim\n    loss_sim = K.mean(loss_sim, axis=-1)\n\n    # Norm loss\n    norm_fg = K.cast(sim_flat > 0, dtype='float32')\n    norm_bg = K.cast(sim_flat < 0, dtype='float32')\n\n    norm_fg_mean = K.sum(norm_fg * feature_norm_minmax_flat[..., tf.newaxis], axis=1) / (K.sum(norm_fg, axis=1) + eps)\n    norm_bg_mean = K.sum(norm_bg * feature_norm_minmax_flat[..., tf.newaxis], axis=1) / (K.sum(norm_bg, axis=1) + eps)\n\n    loss_norm = K.cast(target, dtype='float32') * (norm_bg_mean - norm_fg_mean)\n\n    # per sample loss norm\n    loss_norm = K.sum(loss_norm, axis=1) / K.cast(tf.math.count_nonzero(loss_norm, axis=1), dtype='float32')\n    # scalar loss norm\n    loss_norm = K.mean(loss_norm, axis=-1)\n\n    return loss_sim, loss_norm","metadata":{"id":"nBGaHEN8wVlD","execution":{"iopub.status.busy":"2023-10-14T00:48:06.141182Z","iopub.execute_input":"2023-10-14T00:48:06.141920Z","iopub.status.idle":"2023-10-14T00:48:06.159019Z","shell.execute_reply.started":"2023-10-14T00:48:06.141879Z","shell.execute_reply":"2023-10-14T00:48:06.157766Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## MaxBoxAccV2 callback","metadata":{"id":"7eyl0MsptkJl"}},{"cell_type":"code","source":"%cd /kaggle/working/wsolevaluation\nfrom evaluation import evaluate_wsol","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cVgIgZe5x118","outputId":"f9f944b8-a210-4b13-be3a-2dd0d9dbc32b","execution":{"iopub.status.busy":"2023-10-14T00:48:06.160477Z","iopub.execute_input":"2023-10-14T00:48:06.161818Z","iopub.status.idle":"2023-10-14T00:48:06.307741Z","shell.execute_reply.started":"2023-10-14T00:48:06.161781Z","shell.execute_reply":"2023-10-14T00:48:06.306602Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"/kaggle/working/wsolevaluation\n","output_type":"stream"}]},{"cell_type":"code","source":"class EvaluateMaxBoxAccV2Callback(keras.callbacks.Callback):\n    def __init__(\n        self,\n        data,\n        scoremap_root,\n        metadata_root,\n        mask_root,\n        dataset_name,\n        split,\n        cam_curve_interval,\n        multi_iou_eval,\n        multi_contour_eval,\n        iou_threshold_list=[30, 50, 70]\n    ):\n        self.data = data\n        self.scoremap_root = scoremap_root\n        self.metadata_root = metadata_root\n        self.mask_root = mask_root\n        self.dataset_name = dataset_name\n        self.split = split\n        self.cam_curve_interval = cam_curve_interval\n        self.multi_iou_eval = multi_iou_eval\n        self.multi_contour_eval = multi_contour_eval\n        self.iou_threshold_list = iou_threshold_list\n\n    def on_epoch_end(self, epoch, logs=None):\n        if epoch % 5 != 0:\n            return\n        \n        for batch_idx, batch in self.data.enumerate():\n            x, y = batch\n\n            y_pred = model(x, return_cam=True, target=y['classes'])\n\n            for i, (image, cams_per_image, prediction, gt, image_id) in enumerate(zip(x, y_pred['cam'], y_pred['classes'], y['classes'], y['id'])):\n                shape = image.shape\n\n                prediction = prediction.numpy()\n\n                prediction_rounded = np.flatnonzero(np.round(prediction))\n\n                for target_class, norm_heatmap in cams_per_image.items():\n                    # Set up path for saving\n                    image_id = image_id.numpy().decode('ASCII')\n                    name = os.path.basename(image_id)\n                    saved_dir = os.path.join(self.scoremap_root, os.path.dirname(image_id))\n                    cam_path = os.path.join(saved_dir, name)\n\n                    if not os.path.exists(os.path.dirname(cam_path)):\n                        os.makedirs(os.path.dirname(cam_path))\n\n                    # Save min max normalized heatmap\n                    np.save(cam_path, norm_heatmap)\n\n        logs['val_localization'] = evaluate_wsol(\n            scoremap_root=self.scoremap_root,\n            metadata_root=self.metadata_root,\n            mask_root=self.mask_root,\n            dataset_name=self.dataset_name,\n            split=self.split,\n            cam_curve_interval=self.cam_curve_interval,\n            multi_iou_eval=self.multi_iou_eval,\n            multi_contour_eval=self.multi_contour_eval,\n            iou_threshold_list=self.iou_threshold_list\n        )\n\n        os.system('rm -rf /kaggle/working/cam')","metadata":{"id":"8M9Q54aFtxco","execution":{"iopub.status.busy":"2023-10-14T00:48:46.884090Z","iopub.execute_input":"2023-10-14T00:48:46.884452Z","iopub.status.idle":"2023-10-14T00:48:46.895872Z","shell.execute_reply.started":"2023-10-14T00:48:46.884425Z","shell.execute_reply":"2023-10-14T00:48:46.894491Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## Get model","metadata":{"id":"5g3GCSK8EYIe"}},{"cell_type":"code","source":"class WSOD_Resnet50(keras.Model):\n    def __init__(\n            self,\n            input_shape,\n            num_classes = 1000,\n            drop_threshold = 0.7,\n            drop_prob = 1.0,\n            sim_fg_thres = 0.4,\n            sim_bg_thres = 0.2,\n            loss_ratio_sim = 0.5,\n            loss_ratio_norm = 0.05,\n            loss_ratio_drop = 2.0,\n        ):\n        self.num_classes = num_classes\n        self.sim_fg_thres = sim_fg_thres\n        self.sim_bg_thres = sim_bg_thres\n        self.loss_ratio_sim = loss_ratio_sim\n        self.loss_ratio_norm = loss_ratio_norm\n        self.loss_ratio_drop = loss_ratio_drop\n\n        super().__init__()\n        # Define model\n        resnet50 = keras.applications.ResNet50(\n            include_top=False,\n            weights=\"imagenet\",\n            input_tensor=None,\n            input_shape=input_shape,\n            pooling='None'\n        )\n        model_config = resnet50.get_config()\n\n        for i, layer_config in enumerate(model_config['layers']):\n            if  re.match(r'^conv[45].*conv$', layer_config['config']['name']):\n                model_config['layers'][i]['config']['strides'] = (1, 1)\n\n        model = keras.Model.from_config(model_config)\n        model.set_weights(resnet50.get_weights())\n\n        self.resnet_first_half = keras.models.Model(\n                                model.input,\n                                model.get_layer('conv5_block1_out').output,\n                            )\n\n        self.att_drop = AttentiveDropout(drop_threshold, drop_prob)\n\n        self.resnet_second_half = keras.models.Model(\n                                model.get_layer('conv5_block2_1_conv').input,\n                                model.get_layer('conv5_block3_out').output,\n                            )\n        self.gap = keras.layers.GlobalAveragePooling2D()\n        self.classification = keras.layers.Dense(num_classes, activation = 'softmax', name='classification')\n\n    def get_cam(self, input, feature, targets):\n        def normalize_minmax(cam):\n            if np.isnan(cam).any():\n                return np.zeros_like(cam)\n\n            if cam.min() == cam.max():\n                return np.zeros_like(cam)\n\n            return (cam - cam.min()) / (cam.max() - cam.min())\n\n        cam_batch = []\n\n        for image, image_feature, target in zip(input, feature, targets):\n            shape = image.shape\n\n            target = target.numpy()\n            target = np.round(target)\n\n            cams_per_image = {}\n\n            for c in np.flatnonzero(target):\n                maps = tf.tensordot(\n                    self.classification.weights[0][:, c], # shape (C, )\n                    tf.transpose(image_feature, [2, 0, 1]), # shape (H, W, C) -> (C, H, W)\n                    1\n                )\n\n                heatmap = cv2.resize(maps.numpy(), (shape[1], shape[0]), interpolation = cv2.INTER_CUBIC)\n\n                norm_heatmap = normalize_minmax(heatmap)\n\n                # norm_heatmap = cv2.normalize(heatmap, None, alpha = 1, beta = 0, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_32F)\n                # norm_heatmap = norm_heatmap.astype(np.uint8)\n\n                cams_per_image[c] = norm_heatmap\n\n            cam_batch.append(cams_per_image)\n\n        return cam_batch\n\n\n    def call(self, input, training=False, return_cam=False, target=None):\n        # Model pipeline\n        x = keras.applications.resnet.preprocess_input(input)\n        x = self.resnet_first_half(x, training=training)\n\n        # Temp variable for calculating loss while training\n        erased_feature = tf.identity(x)\n\n        # Continue classification branch\n        x = self.resnet_second_half(x, training=training)\n        feature = tf.identity(x)\n        x = self.gap(x)\n\n        class_output = self.classification(x)\n\n        result = {}\n        result.update({\n            'classes': class_output\n        })\n\n        if training:\n            # Branch: Consistency with attentive dropout\n            erased_feature = self.att_drop(erased_feature)\n            erased_feature = self.resnet_second_half(erased_feature, training=training)\n\n            result.update({\n                'erased_feature': erased_feature,\n                'feature': feature\n            })\n\n        if return_cam:\n            if target is None:\n                raise ValueError('Target is not provided')\n\n            batch_cam = self.get_cam(input, feature, target)\n\n            result.update({\n                'cam': batch_cam\n            })\n\n        return result","metadata":{"id":"c4KP9xMTLOpd","execution":{"iopub.status.busy":"2023-10-14T00:48:47.705349Z","iopub.execute_input":"2023-10-14T00:48:47.705720Z","iopub.status.idle":"2023-10-14T00:48:47.726573Z","shell.execute_reply.started":"2023-10-14T00:48:47.705691Z","shell.execute_reply":"2023-10-14T00:48:47.725309Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"model = WSOD_Resnet50(\n            input_shape = input_shape,\n            num_classes = num_classes,\n            drop_threshold = drop_threshold,\n            drop_prob = drop_prob,\n            sim_fg_thres = sim_fg_thres,\n            sim_bg_thres = sim_bg_thres,\n            loss_ratio_sim = loss_ratio_sim,\n            loss_ratio_norm = loss_ratio_norm,\n            loss_ratio_drop = loss_ratio_drop,\n        )\n# model.build((1,224,224,3))\nmodel.compile(optimizer='sgd')\n# model.compile(\n#     optimizer=keras.optimizers.SGD(\n#         learning_rate=0.002,\n#         momentum=0.9,\n#         weight_decay=0.0001,\n#         nesterov=True\n#     )\n# )","metadata":{"id":"wyKd5rF4-gl1","execution":{"iopub.status.busy":"2023-10-14T00:48:48.005906Z","iopub.execute_input":"2023-10-14T00:48:48.007094Z","iopub.status.idle":"2023-10-14T00:48:51.655830Z","shell.execute_reply.started":"2023-10-14T00:48:48.007020Z","shell.execute_reply":"2023-10-14T00:48:51.654720Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{"id":"_CpCil-rDqHG"}},{"cell_type":"code","source":"!rm /kaggle/working/temp/*","metadata":{"execution":{"iopub.status.busy":"2023-10-14T00:48:51.657766Z","iopub.execute_input":"2023-10-14T00:48:51.658360Z","iopub.status.idle":"2023-10-14T00:48:53.082517Z","shell.execute_reply.started":"2023-10-14T00:48:51.658326Z","shell.execute_reply":"2023-10-14T00:48:53.080852Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"rm: cannot remove '/kaggle/working/temp/*': No such file or directory\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working\n!mkdir /kaggle/working/temp","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FAyL5d2FhQSw","outputId":"e48d8f22-97d2-4112-c85f-9b39ec5970c4","execution":{"iopub.status.busy":"2023-10-14T00:48:53.085118Z","iopub.execute_input":"2023-10-14T00:48:53.085547Z","iopub.status.idle":"2023-10-14T00:48:54.249228Z","shell.execute_reply.started":"2023-10-14T00:48:53.085510Z","shell.execute_reply":"2023-10-14T00:48:54.247909Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"/kaggle/working\nmkdir: cannot create directory ‘/kaggle/working/temp’: File exists\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Callbacks","metadata":{"id":"rSXGpiJWVb8Z"}},{"cell_type":"code","source":"weights_path = '/kaggle/working/temp'\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor = 'val_localization',\n    mode = 'max',\n    min_delta = 0.025,\n    patience = 2,\n    factor = 0.1,\n    verbose = 1,\n    cooldown = 0,\n    min_lr = 0.00000001\n)\n\ncb_early_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_localization',\n    patience=6,\n    min_delta=0.025,\n    verbose=1,\n    mode='max'\n)\n\ncb_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    f'{weights_path}/model.ckpt',\n    monitor='val_localization',\n    save_best_only=True,\n    save_weights_only=True,\n    verbose=1,\n    mode='max'\n)\n\ncsv_logger = tf.keras.callbacks.CSVLogger(f'{weights_path}/training.log', append=True)\n\ncb_evaluate_maxboxaccv2 = EvaluateMaxBoxAccV2Callback(\n    data=val_ds,\n    scoremap_root='/kaggle/working/cam/val',\n    metadata_root='/kaggle/working/wsolevaluation/metadata',\n    mask_root='/kaggle/working/dataset',\n    dataset_name='CUB',\n    split='val',\n    cam_curve_interval=0.005,\n    multi_iou_eval=True,\n    multi_contour_eval=True\n)","metadata":{"id":"Va-LcHWHTOsc","execution":{"iopub.status.busy":"2023-10-14T00:48:54.252797Z","iopub.execute_input":"2023-10-14T00:48:54.253922Z","iopub.status.idle":"2023-10-14T00:48:54.262557Z","shell.execute_reply.started":"2023-10-14T00:48:54.253884Z","shell.execute_reply":"2023-10-14T00:48:54.261416Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"callbacks =  tf.keras.callbacks.CallbackList(\n    [\n        cb_evaluate_maxboxaccv2,\n        cb_checkpoint,\n        csv_logger\n    ],\n    add_history=True, model=model\n)","metadata":{"id":"PWW3YLQt7sU3","execution":{"iopub.status.busy":"2023-10-14T00:48:54.264330Z","iopub.execute_input":"2023-10-14T00:48:54.265104Z","iopub.status.idle":"2023-10-14T00:48:54.278616Z","shell.execute_reply.started":"2023-10-14T00:48:54.265070Z","shell.execute_reply":"2023-10-14T00:48:54.277518Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"### Optimizers","metadata":{"id":"xqR2tWihVh32"}},{"cell_type":"code","source":"classifier_optimizer = keras.optimizers.SGD(\n    learning_rate=0.002,\n    momentum=0.9,\n    weight_decay=0.0001,\n    nesterov=True\n)\n\nfeature_extraction_optimizer = keras.optimizers.SGD(\n    learning_rate=0.002 * 10,\n    momentum=0.9,\n    weight_decay=0.0001,\n    nesterov=True\n)","metadata":{"id":"hkd1xGYbJRMn","execution":{"iopub.status.busy":"2023-10-14T00:48:54.280213Z","iopub.execute_input":"2023-10-14T00:48:54.280911Z","iopub.status.idle":"2023-10-14T00:48:54.296927Z","shell.execute_reply.started":"2023-10-14T00:48:54.280846Z","shell.execute_reply":"2023-10-14T00:48:54.295884Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"### Metrics and Losses","metadata":{"id":"3Y9LXSJsVkL2"}},{"cell_type":"code","source":"cce = keras.losses.CategoricalCrossentropy()\nmae = keras.losses.MeanAbsoluteError()\n\nloss_tracker = keras.metrics.Mean(name='loss')\n\ntop1_acc_train = keras.metrics.TopKCategoricalAccuracy(k=1, name='top1_acc_train')\ntop5_acc_train = keras.metrics.TopKCategoricalAccuracy(k=5, name='top5_acc_train')\n\ntop1_acc_val = keras.metrics.TopKCategoricalAccuracy(k=1, name='top1_acc_val')\ntop5_acc_val = keras.metrics.TopKCategoricalAccuracy(k=5, name='top5_acc_val')","metadata":{"id":"t-xViHskVadN","execution":{"iopub.status.busy":"2023-10-14T00:48:54.298616Z","iopub.execute_input":"2023-10-14T00:48:54.298961Z","iopub.status.idle":"2023-10-14T00:48:54.326904Z","shell.execute_reply.started":"2023-10-14T00:48:54.298929Z","shell.execute_reply":"2023-10-14T00:48:54.325978Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### Define train and val function","metadata":{"id":"lNMpRiWQc7GM"}},{"cell_type":"code","source":"@tf.function\ndef train_step(data, warm):\n    x, y = data\n\n    with tf.GradientTape() as tape:\n        y_pred = model(x, training=True)\n\n        # Calculate loss\n        loss_cce = cce(y['classes'], y_pred['classes'])\n\n        loss_drop = mae(y_pred['feature'], y_pred['erased_feature'])\n\n        loss_sim, loss_norm = get_loss_alignment(\n            feature=y_pred['feature'],\n            weight=model.classification.weights[0],\n            target=y['classes'],\n            sim_bg_thres=model.sim_bg_thres,\n            sim_fg_thres=model.sim_fg_thres,\n        )\n\n        if warm:\n            loss = loss_cce + \\\n                    model.loss_ratio_drop * loss_drop\n        else:\n            loss = loss_cce + \\\n                    model.loss_ratio_drop * loss_drop + \\\n                    model.loss_ratio_sim * loss_sim + \\\n                    model.loss_ratio_norm * loss_norm\n\n    # Get layers' variable\n    feature_extraction_variables = model.trainable_variables[:172]\n    classifier_variables = model.trainable_variables[172:]\n\n    # Compute gradients\n    grads = tape.gradient(loss, feature_extraction_variables + classifier_variables)\n    feature_extraction_grads, classifier_grads = grads[:172], grads[172:]\n    \n    # Update weights\n    feature_extraction_optimizer.apply_gradients(zip(feature_extraction_grads, feature_extraction_variables))\n    classifier_optimizer.apply_gradients(zip(classifier_grads, classifier_variables))\n\n    # Update metrics (includes the metric that tracks the loss)\n    loss_tracker.update_state(loss)\n\n    top1_acc_train.update_state(y['classes'], y_pred['classes'])\n    top5_acc_train.update_state(y['classes'], y_pred['classes'])\n\n    # Return a dict mapping metric names to current value\n    return {\n        'loss': loss_tracker.result(),\n        'top1_acc': top1_acc_train.result(),\n        'top5_acc': top5_acc_train.result(),\n    }\n\n@tf.function\ndef test_step(data):\n    x, y = data\n\n    y_pred = model(x)\n\n    top1_acc_val.update_state(y['classes'], y_pred['classes'])\n    top5_acc_val.update_state(y['classes'], y_pred['classes'])\n\n    return {\n        'top1_acc': top1_acc_val.result(),\n        'top5_acc': top5_acc_val.result(),\n    }","metadata":{"id":"QIuCeF1dIqK5","execution":{"iopub.status.busy":"2023-10-14T00:48:54.328232Z","iopub.execute_input":"2023-10-14T00:48:54.328578Z","iopub.status.idle":"2023-10-14T00:48:54.340538Z","shell.execute_reply.started":"2023-10-14T00:48:54.328545Z","shell.execute_reply":"2023-10-14T00:48:54.339470Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"### Start training","metadata":{"id":"vBK96wL9dAMH"}},{"cell_type":"code","source":"def adjust_learning_rate(epoch):\n    if epoch != 0 and epoch in [41, 61]:\n        K.set_value(classifier_optimizer.learning_rate,classifier_optimizer.learning_rate * 0.2)\n        K.set_value(feature_extraction_optimizer.learning_rate, feature_extraction_optimizer.learning_rate * 0.2)","metadata":{"id":"tWnCZiAdZVkw","execution":{"iopub.status.busy":"2023-10-14T00:48:54.342102Z","iopub.execute_input":"2023-10-14T00:48:54.342734Z","iopub.status.idle":"2023-10-14T00:48:54.353676Z","shell.execute_reply.started":"2023-10-14T00:48:54.342699Z","shell.execute_reply":"2023-10-14T00:48:54.352516Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def print_metrics(metrics):\n    maxlen = max([len(key) for key in metrics.keys()])\n    print(\"\\tMetrics:\")\n    print(\"\\t\" + \"-\" * (maxlen + 1))\n    for k, v in metrics.items():\n        print(f\"\\t{k.ljust(maxlen+1)}: {v.numpy():0.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-14T00:48:54.356673Z","iopub.execute_input":"2023-10-14T00:48:54.357312Z","iopub.status.idle":"2023-10-14T00:48:54.365887Z","shell.execute_reply.started":"2023-10-14T00:48:54.357277Z","shell.execute_reply":"2023-10-14T00:48:54.364720Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"NUM_EPOCHS = 50\n\ncallbacks.on_train_begin()\n\nlogs = None\n\nfor epoch in range(1, NUM_EPOCHS + 1):\n    print(f'Epoch: {epoch}')\n    warm = True if epoch < 20 else False\n\n    # train\n    print('Start training')\n    callbacks.on_epoch_begin(epoch)\n\n    for step, data in tqdm.tqdm(train_ds.enumerate()):\n        callbacks.on_train_batch_begin(step)\n\n        adjust_learning_rate(epoch + 1)\n        logs = train_step(data, warm)\n\n        callbacks.on_train_batch_end(step + 1, logs)\n\n        \n    loss_tracker.reset_states()\n    top1_acc_train.reset_states()\n    top5_acc_train.reset_states()\n    \n    epoch_logs = copy.copy(logs)\n\n    # evaluate\n    val_logs = {}\n\n    print('Start evaluating')\n    callbacks.on_test_begin()\n\n    for step, data in val_ds.enumerate():\n        callbacks.on_test_batch_begin(step)\n\n        val_logs = test_step(data)\n        \n        callbacks.on_test_batch_end(step)\n\n    callbacks.on_test_end(val_logs)\n    \n    top1_acc_val.reset_states()\n    top5_acc_val.reset_states()\n\n    val_logs = {\n        \"val_\" + name: val for name, val in val_logs.items()\n    }\n    \n    epoch_logs.update(val_logs)\n\n    callbacks.on_epoch_end(epoch, epoch_logs)\n    print_metrics(epoch_logs)\n    \n\ncallbacks.on_train_end(val_logs)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5w5hwj2m6I_R","outputId":"2305ea6b-9586-455b-faf9-fc6a2bb8d7e6","scrolled":true,"execution":{"iopub.status.busy":"2023-10-14T00:48:55.584626Z","iopub.execute_input":"2023-10-14T00:48:55.584995Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch: 1\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [03:21,  1.07s/it]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : 5.07\n\ttop1_acc     : 0.03\n\ttop5_acc     : 0.12\n\tval_top1_acc : 0.08\n\tval_top5_acc : 0.24\nEpoch: 2\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:02,  1.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : 3.57\n\ttop1_acc     : 0.20\n\ttop5_acc     : 0.49\n\tval_top1_acc : 0.13\n\tval_top5_acc : 0.38\nEpoch: 3\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:02,  1.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : 2.43\n\ttop1_acc     : 0.42\n\ttop5_acc     : 0.75\n\tval_top1_acc : 0.26\n\tval_top5_acc : 0.58\nEpoch: 4\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:21,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : 1.72\n\ttop1_acc     : 0.59\n\ttop5_acc     : 0.88\n\tval_top1_acc : 0.28\n\tval_top5_acc : 0.60\nEpoch: 5\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:01,  1.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\nLoading and evaluating cams.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"localization: 41.699999999999996\n\nEpoch 6: val_localization improved from -inf to 41.70000, saving model to /kaggle/working/temp/model.ckpt\n\tMetrics:\n\t-------------\n\tloss         : 1.22\n\ttop1_acc     : 0.71\n\ttop5_acc     : 0.94\n\tval_top1_acc : 0.30\n\tval_top5_acc : 0.61\nEpoch: 6\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:01,  1.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : 0.87\n\ttop1_acc     : 0.81\n\ttop5_acc     : 0.98\n\tval_top1_acc : 0.37\n\tval_top5_acc : 0.68\nEpoch: 7\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:21,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : 0.59\n\ttop1_acc     : 0.89\n\ttop5_acc     : 0.99\n\tval_top1_acc : 0.40\n\tval_top5_acc : 0.74\nEpoch: 8\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:01,  1.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : 0.40\n\ttop1_acc     : 0.94\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.43\n\tval_top5_acc : 0.76\nEpoch: 9\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:01,  1.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : 0.28\n\ttop1_acc     : 0.97\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.52\n\tval_top5_acc : 0.83\nEpoch: 10\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:02,  1.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\nLoading and evaluating cams.\nlocalization: 51.033333333333324\n\nEpoch 11: val_localization improved from 41.70000 to 51.03333, saving model to /kaggle/working/temp/model.ckpt\n\tMetrics:\n\t-------------\n\tloss         : 0.18\n\ttop1_acc     : 0.99\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.57\n\tval_top5_acc : 0.86\nEpoch: 11\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:02,  1.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : 0.15\n\ttop1_acc     : 1.00\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.61\n\tval_top5_acc : 0.87\nEpoch: 12\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:01,  1.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : 0.12\n\ttop1_acc     : 1.00\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.60\n\tval_top5_acc : 0.87\nEpoch: 13\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:01,  1.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : 0.12\n\ttop1_acc     : 1.00\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.58\n\tval_top5_acc : 0.87\nEpoch: 14\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:01,  1.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : 0.11\n\ttop1_acc     : 1.00\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.61\n\tval_top5_acc : 0.88\nEpoch: 15\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:02,  1.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\nLoading and evaluating cams.\nlocalization: 47.06666666666666\n\nEpoch 16: val_localization did not improve from 51.03333\n\tMetrics:\n\t-------------\n\tloss         : 0.11\n\ttop1_acc     : 1.00\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.63\n\tval_top5_acc : 0.86\nEpoch: 16\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:02,  1.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : 0.11\n\ttop1_acc     : 1.00\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.61\n\tval_top5_acc : 0.86\nEpoch: 17\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:21,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : 0.10\n\ttop1_acc     : 1.00\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.61\n\tval_top5_acc : 0.87\nEpoch: 18\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:01,  1.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : 0.10\n\ttop1_acc     : 1.00\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.61\n\tval_top5_acc : 0.86\nEpoch: 19\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:02,  1.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : 0.10\n\ttop1_acc     : 1.00\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.60\n\tval_top5_acc : 0.86\nEpoch: 20\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:25,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\nLoading and evaluating cams.\nlocalization: 48.06666666666666\n\nEpoch 21: val_localization did not improve from 51.03333\n\tMetrics:\n\t-------------\n\tloss         : -0.00\n\ttop1_acc     : 1.00\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.61\n\tval_top5_acc : 0.87\nEpoch: 21\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:06,  1.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : -0.01\n\ttop1_acc     : 1.00\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.62\n\tval_top5_acc : 0.87\nEpoch: 22\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:06,  1.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : -0.01\n\ttop1_acc     : 1.00\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.62\n\tval_top5_acc : 0.87\nEpoch: 23\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:06,  1.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : -0.02\n\ttop1_acc     : 1.00\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.61\n\tval_top5_acc : 0.86\nEpoch: 24\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:06,  1.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : -0.03\n\ttop1_acc     : 1.00\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.61\n\tval_top5_acc : 0.86\nEpoch: 25\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:21,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\nLoading and evaluating cams.\nlocalization: 48.73333333333333\n\nEpoch 26: val_localization did not improve from 51.03333\n\tMetrics:\n\t-------------\n\tloss         : -0.03\n\ttop1_acc     : 1.00\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.62\n\tval_top5_acc : 0.87\nEpoch: 26\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:21,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : -0.04\n\ttop1_acc     : 1.00\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.62\n\tval_top5_acc : 0.87\nEpoch: 27\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:06,  1.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : -0.04\n\ttop1_acc     : 1.00\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.62\n\tval_top5_acc : 0.87\nEpoch: 28\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:06,  1.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : -0.04\n\ttop1_acc     : 1.00\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.62\n\tval_top5_acc : 0.87\nEpoch: 29\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:06,  1.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : -0.05\n\ttop1_acc     : 1.00\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.62\n\tval_top5_acc : 0.87\nEpoch: 30\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:06,  1.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\nLoading and evaluating cams.\nlocalization: 49.866666666666674\n\nEpoch 31: val_localization did not improve from 51.03333\n\tMetrics:\n\t-------------\n\tloss         : -0.05\n\ttop1_acc     : 1.00\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.60\n\tval_top5_acc : 0.85\nEpoch: 31\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:21,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : 0.01\n\ttop1_acc     : 0.99\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.40\n\tval_top5_acc : 0.70\nEpoch: 32\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:06,  1.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : 0.21\n\ttop1_acc     : 0.95\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.18\n\tval_top5_acc : 0.40\nEpoch: 33\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:06,  1.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : 0.35\n\ttop1_acc     : 0.90\n\ttop5_acc     : 0.99\n\tval_top1_acc : 0.37\n\tval_top5_acc : 0.68\nEpoch: 34\nStart training\n","output_type":"stream"},{"name":"stderr","text":"188it [02:06,  1.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Start evaluating\n\tMetrics:\n\t-------------\n\tloss         : 0.14\n\ttop1_acc     : 0.96\n\ttop5_acc     : 1.00\n\tval_top1_acc : 0.47\n\tval_top5_acc : 0.77\nEpoch: 35\nStart training\n","output_type":"stream"},{"name":"stderr","text":"149it [01:40,  1.48it/s]","output_type":"stream"}]},{"cell_type":"code","source":"logs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Show heatmap","metadata":{"id":"hH2JMyf8-_Nx"}},{"cell_type":"code","source":"model.load_weights('/kaggle/working/temp/model.ckpt')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qbAQW56-AO_2","outputId":"557d718d-69f9-45e7-e750-b16169893cdb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(val_ds)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RoVd3ypzAXxx","outputId":"0dbb0763-25fd-42a1-e891-7137ec2577dc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_ids = [\n    \"Aeroplane\",\n    \"Bicycle\",\n    \"Bird\",\n    \"Boat\",\n    \"Bottle\",\n    \"Bus\",\n    \"Car\",\n    \"Cat\",\n    \"Chair\",\n    \"Cow\",\n    \"Dining Table\",\n    \"Dog\",\n    \"Horse\",\n    \"Motorbike\",\n    \"Person\",\n    \"Potted Plant\",\n    \"Sheep\",\n    \"Sofa\",\n    \"Train\",\n    \"Tvmonitor\",\n    \"Total\",\n]\nclass_mapping = dict(zip(range(len(class_ids)), class_ids))","metadata":{"id":"qxpkthHDA47j","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch_idx, batch in val_ds.enumerate():\n    # if batch_idx != 1:\n    #     continue\n\n    x, y = batch\n\n    y_pred = model(x, return_cam=True, target=y['classes'])\n\n    cam_per_batch = []\n\n\n    for i, (image, cams_per_image, prediction, gt) in enumerate(zip(x, y_pred['cam'], y_pred['classes'], y['classes'])):\n        print('----------')\n        print(i)\n        shape = image.shape\n\n        prediction = prediction.numpy()\n\n        prediction_rounded = np.flatnonzero(np.round(prediction))\n\n        if len(prediction_rounded) == 0:\n            print('no prediction')\n            prediction_rounded = np.argsort(-prediction)[:5]\n\n        print(f'\\tPrediction:   {prediction_rounded}')\n        print(f'\\tScore:        {np.array2string(prediction[prediction_rounded], precision=2, floatmode=\"fixed\")}')\n        print(f'\\tGround_truth: {np.flatnonzero(gt)}')\n\n        print()\n\n        for target_class, norm_heatmap in cams_per_image.items():\n            print(f'\\t{target_class}')\n\n            heatmap = cv2.applyColorMap(norm_heatmap, cv2.COLORMAP_JET)\n            superimposed = cv2.addWeighted(heatmap, 0.5, image.numpy().astype(np.uint8), 0.5, 0)\n\n            cv2_imshow(superimposed)\n    break","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"bYW6gYXv_Cws","outputId":"c793a307-90b3-41c2-b733-e81891fb4bf9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Maxboxaccv2","metadata":{"id":"gT5gPrISz6Rs"}},{"cell_type":"code","source":"model.load_weights('/kaggle/working/temp/model.ckpt')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bVbhlWMgzzlD","outputId":"ed7bae6f-4434-4eed-b1e6-025d124522c0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SAVED_CAM_PATH = '/kaggle/working/cam'\nSPLIT = 'val'","metadata":{"id":"fMmCi8dX0r73"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/cam","metadata":{"id":"Y-eUHkWQ2ZWf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch_idx, batch in val_ds.enumerate():\n    # if batch_idx != 1:\n    #     continue\n\n    x, y = batch\n\n    y_pred = model(x, return_cam=True, target=y['classes'])\n\n    for i, (image, cams_per_image, prediction, gt, image_id) in enumerate(zip(x, y_pred['cam'], y_pred['classes'], y['classes'], y['id'])):\n        shape = image.shape\n\n        prediction = prediction.numpy()\n\n        prediction_rounded = np.flatnonzero(np.round(prediction))\n\n        for target_class, norm_heatmap in cams_per_image.items():\n            # Set up path for saving\n            image_id = image_id.numpy().decode('ASCII')\n            name = os.path.basename(image_id)\n            saved_dir = os.path.join(SAVED_CAM_PATH, SPLIT, os.path.dirname(image_id))\n            cam_path = os.path.join(saved_dir, name)\n\n            # plt.imshow(norm_heatmap)\n\n            if not os.path.exists(os.path.dirname(cam_path)):\n                os.makedirs(os.path.dirname(cam_path))\n\n            # Save min max normalized heatmap\n            np.save(cam_path, norm_heatmap)\n\n            # # Superimpose heatmap on original image\n            # norm_heatmap = norm_heatmap.astype(np.uint8)\n            # heatmap = cv2.applyColorMap(norm_heatmap, cv2.COLORMAP_JET)\n            # superimposed = cv2.addWeighted(heatmap, 0.5, image.numpy().astype(np.uint8), 0.5, 0)\n\n            # cv2_imshow(superimposed)\n            # cv2.imwrite(f'{cam_path}.png', superimposed)\n\n    #         break\n    #     break\n    # break","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":434},"id":"Ws3Y_j9tzPEf","outputId":"94028966-a612-4812-abf5-f6a87838b888"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\n\nlen(glob.glob('/kaggle/working/cam/**/*.jpg.npy', recursive=True))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6DbFYG3N2HEY","outputId":"c00ead28-b924-4088-ca76-24dba29b4b30"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/wsolevaluation\n!python evaluation.py --scoremap_root=/kaggle/working/cam/val \\\n                     --metadata_root=metadata/ \\\n                     --mask_root=dataset/ \\\n                     --dataset_name=CUB \\\n                     --split=val \\\n                     --cam_curve_interval=0.005 \\\n                     --multi_iou_eval \\\n                     --multi_contour_eval","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UmLsWTrw2spL","outputId":"085cb4ac-a7a4-42a2-a895-c3720dc6717b"},"execution_count":null,"outputs":[]}]}